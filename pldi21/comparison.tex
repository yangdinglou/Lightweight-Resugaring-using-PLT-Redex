%!TEX root = ./main.tex
\section{Discussion}
\label{sec5}

\subsection{Model Assumption and A Black-Box Extension}
\label{sec5.1}


As we mentioned in the introduction (Section \ref{mark:mention}), our approach assumes a more specific model (evaluation rules) compared to the existing approach (black-box stepper). Here is a small gap between the motivation of the existing approach and ours---the existing approach focused mainly on a tool for existing language, while our approach considered more on a meta-level feature for language implementation. The examples in Section \ref{sec:recursiveSugar} have shown how the lazy desugaring solves some problems in practice.

In addition, as what we need for the lazy desugaring is just the computational order of the syntactic sugar, we can make an extension for the resugaring algorithm to work with only a black-box core language stepper. The most important difference between the black-box stepper and the evaluation rules is the computational order---while the same language behaves uniquely, the evaluation rules can show the computational order statically (without running the program). So when meeting the black-box stepper for the core language, we can just use some simple program to "get" the computational order of the core language as the following example  shows: we simply let the sub-expressions of a \m{Head} be some reducible expressions and test the computational order.

\begin{center}\footnotesize
	\Code{(if $\m{tmpe}_1$ $\m{tmpe}_2$ $\m{tmpe}_3$)}\\ $\Downarrow_{stepper}$\\ \Code{\qquad\qquad\qquad\qquad\;\;(if $\m{tmpe}_1$' $\m{tmpe}_2$ $\m{tmpe}_3$)}\note{//getting a context rule}\\ $\Downarrow_{getnext}$\\ \Code{(if $\m{tmpv}_1$ $\m{tmpe}_2$ $\m{tmpe}_3$)}\\ $\Downarrow_{stepper}$\\ \qquad\Code{$\m{tmpe}_i$}\note{//no more rules}\\
\end{center}


But that's not enough---the core language and the surface language cannot be mixed easily because of the lack of evaluation rules for the core language. We should do the same try during the evaluation to make the core language's stepper useful when meeting some surface language's expression. Here we give a dynamic on-step reduction of the mixed language. Note that here we only define the reduction for unnested syntactic sugar for convenience. It is easy to extend to nested sugar (but so huge to express). 

\begin{Def}[Dynamic mixed language's one-step reduction $\redm{}{}$] Defined in Fig.  \ref{fig:dynamic}.
\end{Def}
\begin{figure*}[t]\footnotesize
\infrule[CoreRed]
{ \forall~i.~e_i\in \m{CoreExp}\\
\redc{(\m{CoreHead}~e_1~\ldots~e_n)}{e'}}
{\redm{(\m{CoreHead}~e_1~\ldots~e_n)}{e'}}
\infrule[CoreExt1]
{ \forall~i.~tmp_i= (e_i \in \m{SurfExp}~?~\m{tmpe}~:~e_i)\\
\redc{(\m{CoreHead}~tmp_1~\ldots~tmp_i~\ldots~tmp_n)}{(\m{CoreHead}~tmp_1~\ldots~tmp_i'~\ldots~tmp_n)}}
{\redm{(\m{CoreHead}~e_1~\ldots~e_i~\ldots~e_n)}{(\m{CoreHead}~e_1~\ldots~e_i'~\ldots~e_n)}\\where~\redm{e_i}{e_i'}}
\infrule[CoreExt2]
{ \forall~i.~tmp_i= (e_i \in \m{SurfExp}~?~\m{tmpe}~:~e_i)\\
\redc{(\m{CoreHead}~tmp_1~\ldots~tmp_n)}{e'}~\note{// not reduced in sub-expressions}}
{\redm{(\m{CoreHead}~e_1~\ldots~e_n)}{e'[e_1/tmp_1~\ldots~e_n/tmp_n]}}
\infrule[SurfRed1]
{\drule{(\m{SurfHead}~x_1~\ldots~x_i~\ldots~x_n)}{e}~\\
\exists i.\, \redm{e[e_1/x,\ldots,e_i/x_i,\ldots,e_n/x_n]}{e[e_1/x,\ldots,e_i'/x_i,\ldots,e_n/x_n]}~\&~
\redm{e_i}{e_i''}
}
{\redm{(\m{SurfHead}~e_1~\ldots~e_i~\ldots~e_n)}{(\m{SurfHead}~e_1~\ldots~e_i''~\ldots~e_n)}}
\infrule[SurfRed2]
{\drule{(\m{SurfHead}~x_1~\ldots~x_i~\ldots~x_n)}{e}\\
\not \exists i.\, \redm{e[e_1/x_1,\ldots,e_i/x_i,\ldots,e_n/x_n]}{e[e_1/x_1,\ldots,e_i'/x_i,\ldots,e_n/x_n]~\&~
\redm{e_i}{e_i''}}
}
{\redm{(\m{SurfHead}~e_1~\ldots~e_i~\ldots~e_n)}{e[e_1/x_1,\ldots,e_i/x_i,\ldots,e_n/x_n]}}
\footnotesize{where~\m{tmpe}~is~any~reduciable~\m{CoreExp}~expression}
\caption{Dynamic Reduction}
\label{fig:dynamic}
\end{figure*}

Putting them in words. For expression \Code{(SurfHead $e_1$ $\ldots$ $e_n$)}, as we discussed in Section \ref{mark:correct}, only reduction on the $e_i$ of the sugar's $LHS$ will not destroy the $RHS$'s form. So we can just take a try after the expansion of \m{SurfHead}. 

For an expression \Code{(CoreHead $e_1$ $\ldots$ $e_n$)}  whose sub-expressions contain \m{SurfExp}, replacing all \m{SurfExp} sub-expressions with any reducible core language's expression $\m{tmpe}_i$ . Then getting a result after inputting the new expression $e'$ to the original black-box stepper. Then two possible cases come.

If reduction appears at a sub-expression at $\m{tmpe}_i$'s location, then the stepper with the extension should return \Code{(CoreHead $e_1$ $\ldots$ $e_i'$ $\ldots$ $e_n$)}, where $e_i'$ is $e_i$ after the mixed language's one-step reduction ($\redm{}{}$) as the following example (rule $\mathtt{CoreExt1}$)
\begin{center}\scriptsize
	\Code{(if (and e1 e2) (or e1 e2) \#f)}\\ $\Downarrow_{replace}$\\ \Code{(if (not \#t) (not \#t) \#f)}\\ $\;\;\Downarrow_{blackbox}$\\ \Code{(if \#f (not \#t) \#f)}\\ $\quad\Downarrow_{reduction}$\\ \Code{(if (if e1 e2 \#f) (or e1 e2) \#f)}
\end{center}

Otherwise (no reduction on $\m{tmp}_i$), the stepper should return \Code{$e'$}, with all the replaced sub-expressions replacing back (rule $\mathtt{CoreExt2}$).
\begin{center}\scriptsize
	\Code{(if \#t (and ...) (or ...))}\\ $\Downarrow_{replace}$ \\\Code{ (if \#t $\m{tmpe}_2$ $\m{tmpe}_3$)}\\ $\;\;\Downarrow_{blackbox}$\\  \Code{$\m{tmpe}_2$}\\ $\quad\;\;\;\Downarrow_{replaceback}$\\ \Code{(and ...)}
\end{center}
We call the extension "one-step-try", because it tries one step on the expression in the black-box stepper. The extension will not violate the properties of the original core language's evaluator. It is obvious that the evaluator with the extension will reduce at the sub-expression as it needs in the core language, if the reduction appears in a sub-expression. The stepper with extension behaves the same as mixing the evaluation rules of the core language and desugaring rules of surface language.

But something goes wrong when substitution takes place during \m{CoreExt2}. For a program like \Code{(let (x 2) (Sugar x y))} as an example, it should reduce to \Code{(Sugar 2 y)} by the \m{CoreRed2} rule, but got \Code{(Sugar x y)} by the \m{CoreExt2} rule. So when using the extension of black-box stepper's rule (\m{ExtRed2}), we need some other information about in which sub-expression a substitution will occur. Then for these sub-expressions, we need to do the same substitution before replacing back. The substitution can be got by a similar idea as the dynamic reduction in our simple core language's setting. For example, we know the third sub-expression of an expression headed with \m{let} is to be substituted. we should first try \Code{(let (x 2) x)}, \Code{(let (x 2) y)} in one-step reduction to get the substitution [2/x], then, getting \Code{(Sugar 2 y)}.

Then for any sugar expression, we can process them dynamically by "one-step-try" like the example in Fig.  \ref{example:try}. (The bold \m{Head} means trying on this expression.)
\example{
\[
{\footnotesize
	\begin{array}{lcl}
	\text{resugaring}&&\text{one-step-try}\\
	\Code{({\bfseries And} (Or \#t \#f)}&\xrightarrow{try}&\Code{(if ({\bfseries Or} \#t \#f)}\\
	\Code{\qquad\hspace{0.5em}(And \#f \#t))}&&\Code{\qquad(And \#f \#t)}\\
	& &\Code{\qquad\#f)}\\
	\qquad\quad\dashdownarrow& &\qquad\qquad\downarrow\\
	\Code{(And ({\bfseries Or} \#t \#f)}& &\Code{(And ({\bfseries if} \#t \#t \#f)}\\
	\Code{\qquad\hspace{0.5em}(And \#f \#t))}&&\Code{\qquad\hspace{0.5em}(And \#f \#t))}\\
	\qquad\quad\dashdownarrow& &\qquad\qquad\downarrow\\
	\Code{({\bfseries And} \#t}&\xrightarrow{try}&\Code{({\bfseries if} \#t}\\
	\Code{\qquad\hspace{0.5em}(And \#f \#t))}&&\Code{\qquad\hspace{0.5em}(And \#f \#t)}\\
	& &\Code{\qquad\hspace{0.5em}\#f)}\\
	\qquad\quad\dashdownarrow& &\qquad\qquad\downarrow\\
	\Code{({\bfseries And} \#f \#t)}&\xrightarrow{try}&\Code{({\bfseries if} \#f \#t \#f)}\\
	\qquad\quad\dashdownarrow& &\\
	\Code{\#f}& &\\
\end{array}
}
\]
}{Example of One-Step-Try}{example:try}



\subsection{Properties and Trade-off}
\label{mark:correctness}

The existing resugaring approaches \cite{resugaring,hygienic} proposed the following three properties to define the correctness.

\begin{quote}
Emulation:
Each term in the generated surface evaluation sequence desugars into the core term which it is meant to represent.\\
Abstraction:
Code introduced by desugaring is never revealed in the surface evaluation sequence, and code originating from the original input program is never hidden by resugaring.\\
Coverage: Resugaring is attempted on every core step, and as few core steps are skipped as possible.\\
\end{quote}
Here we will show what are the similarities and differences between theirs and our properties.

\emph{Emulation}: The properties in Section \ref{sec3} is just the same as the emulation property. It is the most basic one.

\emph{Abstraction and Coverage}: Our reduction in the mixed language has some similarities to theirs. But since our framework has no execution for the fully desugared program and no reverse desugaring, there are some differences in details.

Overall, our approach restricts the output by the \m{Head} of an expression and its sub-expressions. It is quite natural since the motivation of the resugaring is to show useful intermediate sequences, we think it will be better than restricting the output by judging whether the intermediate expressions contain some components desugared from the original program's components. Take the following sugar definitions as an example.
\[
\drule{\Code{(Nor~x~y)}}{\Code{(And~(not~x)~(not~y))}}
\]
\[
\drule{\Code{(And~x~y)}}{\Code{(if~x~y~\false)}}
\]
Then for a logic domain, what should be a resugaring sequence of the program \Code{(not (And (Nor \false~\true) \true))} ?

In our opinion, if the outer \m{not}, \m{And} can be displayed, so they should be after desugared.
The existing approach will produce the sequences as follows.
\begin{footnotesize}
\begin{Codes}
    (not (And (Nor \false \true) \true))
\OneStep{ (not (And \false \true))}
\OneStep{ (not \false)}
\OneStep{ \true}
\end{Codes}
\end{footnotesize}
while ours will produce the following sequences.
\begin{footnotesize}
\begin{Codes}
    (not (And (Nor \false \true) \true))
\OneStep{ (not (And (And (not \false) (not \true)) \true))}
\OneStep{ (not (And (And \true (not \true)) \true))}
\OneStep{ (not (And (not \true) \true))}
\OneStep{ (not (And \false \true))}
\OneStep{ (not \false)}
\OneStep{ \true}
\end{Codes}
\end{footnotesize}

Also, if we want to display the core language's expression only when it is originated from the input program, we can just make a mirror for it as a \m{CommonHead}. For example, when we want to show resugaring sequences of \Code{(And (if (And \#t \#f) ...) ...)}
without showing the \m{if} expression expanded from \m{And}, we only need to set \m{If} as \m{CommonHead} together with its evaluation rules same as \m{if}.

In summary, our approach chooses a slightly different way for the \emph{abstraction} for better \emph{coverage} in the real application.
\subsection{Hygiene}
\label{mark:hygiene}

As an important property for sugar or macro system, we used to think it necessary to achieve hygiene by combining the approach with an existing hygienic desugaring system. But during the experiment, we find it naturally solve the hygienic problem with the original desugaring system in our language setting.

In our approach, the sugar can contain some bindings, written by the core language's \m{let}. The hygienic problem only happens when binders of an expanded sugar conflict with other binders. We classify them into following two cases. Any hygienic problems are composite by the two cases.

The first one is that, a sugar expression exists in binding's evaluation context. For the sugar \m{Or1} with following rule,
\[\drule{\Code{(Or1~$e_1$~$e_2$)}}{\Code{(let (t $e_1$) (if t t $e_2$))}}\]
The program \Code{(let (t \#t) (Or1 \#f t))} is of the case. But because of the context rule of \m{let}, the sugar \m{Or1} will not be expanded before the \m{t} is substituted. So the program reduces to \Code{(Or1 \#f \#t)} first, so avoiding the hygienic problem. Because the bound variables in sugar expressions are only introduced by let-binding, all of them can "delay" the expansion of the syntactic sugar.

The second one is that, a sugar expression which introduced binding by the sugar expansion contains bindings in its sub-expression. For the sugar \m{Subst} with following rule,
\[
\drule{\Code{(Subst $e_1$ $e_2$ $e_3$)}}{\Code{(let ($e_2$ $e_3$) $e_1$)}}
\]
The program \Code{(Subst (+ f (let (f 1) f)) f 5)} is of the case. The sugar introduces a local-binding on the variable \m{f}, with its sub-expression contains multiple \m{f}. By calculating the context rules of \m{Subst}, the sugar has to be expanded after the $e_3$ being a value. After desugaring to \Code{(let (f 5) (+ f (let (f 1) (+ f 1))))},  no hygienic problem will take place because of the capture-avoiding substitution in the core language.

Because of the definition of desugaring in our approach, we cannot achieve hygiene by proving the $\alpha-equivalence$.
Here what we want to show is that, even without complex things like macro systems, scope specification and so on, the lazy desugaring itself will solve the common hygienic problem with carefully-designed core language. And of course the lazy desugaring will also work together with a hygienic desugaring system (e.g., by specific the binding scope \cite{10.5555/1792878.1792884}). Also, the scope inference of syntactic sugar\cite{resugaringscope} provides a good perspective for solving the hygienic problem.
\subsection{Limitation on Presentation}
The context rules of our sugar setting limit the presentation of syntactic sugar. For example, it is difficult to present a sugar with ellipses as pattern variables in our current tool, because the form of its context rule may vary. It is still possible if we add some
restriction (so that the algorithm \ref{alg:f} will work). On the other hand, we can just make it using the list operation just as what the sugar \m{Map, Filter} work. Overall, the presentation of our sugar system is not so flexible, but it won't affect the expressiveness.